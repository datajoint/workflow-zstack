{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DataJoint U24 - Workflow Volume\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Interactively run the workflow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you haven't configured your set up, refer to [01-Configure](./01-Configure.ipynb).\n",
    "- For an overview of the schema, refer to [02-WorkflowStructure](02-WorkflowStructure_Optional.ipynb).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the directory to load the local config, `dj_local_conf.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# change to the upper level folder to detect dj_local_conf.json\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline.py` activates the various schema and declares other required tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "from datetime import datetime\n",
    "from element_zstack.export.bossdb import BossDBUpload\n",
    "from workflow_zstack.pipeline import (\n",
    "    lab,\n",
    "    subject,\n",
    "    session,\n",
    "    scan,\n",
    "    volume,\n",
    "    bossdb,\n",
    "    get_session_directory,\n",
    "    get_volume_root_data_dir,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Manually Inserting Entries\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upstream tables\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can insert entries into `dj.Manual` tables (green in diagrams) by providing values as a dictionary or a list of dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(subject=\"sub1\", sex=\"M\", subject_birth_date=datetime.now()),\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "session_key = dict(\n",
    "    subject=\"sub1\",\n",
    "    session_id=1,\n",
    ")\n",
    "session.Session.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        session_datetime=datetime.now(),\n",
    "    ),\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "session.SessionDirectory.insert1(\n",
    "    dict(**session.Session.fetch1(\"KEY\"), session_dir=\"\"),\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "scan.Scan.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        scan_id=1,\n",
    "        acq_software=\"ScanImage\",\n",
    "    ), skip_duplicates=True\n",
    ")\n",
    "scan_key = (scan.Scan & \"subject = 'sub1'\").fetch1(\"KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Volume.populate(display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a pameter set for segmentation with cellpose in the\n",
    "`SegmentationParamset` table, and insert an entry into the `SegmentationTask`\n",
    "table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.SegmentationParamset.insert_new_params(\n",
    "    segmentation_method=\"cellpose\",\n",
    "    paramset_idx=1,\n",
    "    params=dict(\n",
    "        diameter = 8,\n",
    "        min_size = 2,\n",
    "        do_3d = False,\n",
    "        anisotropy = 0.5,\n",
    "        model_type = \"nuclei\",\n",
    "        channels = [[0, 0]],\n",
    "        z_axis = 0,\n",
    "        skip_duplicates=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.SegmentationTask.insert1(\n",
    "    dict(\n",
    "        scan_key,\n",
    "        paramset_idx=1,\n",
    "        task_mode=\"trigger\",\n",
    "        ), skip_duplicates=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can popluate the `Segmentation` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Segmentation.populate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can upload our data either from the data stored in the table or a path to images. If this entry is already associated with a `SessionDirectory` entry, we'll look for images in this path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bossdb.UploadParamSet.insert_new_params(\n",
    "    paramset_idx=1,\n",
    "    paramset_desc=\"test params\",\n",
    "    params=dict(\n",
    "        voxel_units=\"micrometers\",\n",
    "        voxel_size=[1,1,1],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bossdb.VolumeUploadTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = \"dataJointTestUpload\"\n",
    "exp_name = \"CaImaging\"\n",
    "chn_name = \"test10\"\n",
    "bossdb.VolumeUploadTask.update1(\n",
    "    dict(\n",
    "        scan_key,\n",
    "        paramset_idx=1,\n",
    "        collection_name=col_name,\n",
    "        experiment_name=exp_name,\n",
    "        channel_name=chn_name,\n",
    "        upload_type=\"image\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bossdb.VolumeUploadTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_key = (bossdb.VolumeUploadTask & scan_key & \"channel_name = 'test10'\").fetch(\"KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bossdb.BossDBURLs.populate(upload_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bossdb.BossDBURLs & scan_key).fetch1(\"neuroglancer_url\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how to get permission from APL to upload data. \n",
    "\n",
    "Create a schema to automatically generate neuroglancer link and insert into DJ\n",
    "table. \n",
    "\n",
    "Include BossDBUpload in BossDBURLs as a computed/imported table. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ele')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d00c4ad21a7027bf1726d6ae3a9a6ef39c8838928eca5a3d5f51f3eb68720410"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
