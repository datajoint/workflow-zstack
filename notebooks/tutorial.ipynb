{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Manage volumetric fluorescent microscopy experiments with DataJoint Elements\n",
    "\n",
    "This notebook will walk through processing volumetric two-photon calcium imaging\n",
    "data with the DataJoint Workflow for volumetric image processing. The workflow\n",
    "currently supports volumetric data collected\n",
    "from ScanImage. \n",
    "\n",
    "**Please note that uploading data to BossDB via this pipeline requires an API\n",
    "token which can be obtained by creating an account at\n",
    "[api.bossdb.io](https://api.bossdb.io). You will also need resource manager\n",
    "permissions from the team at [BossDB](https://bossdb.org).**\n",
    "\n",
    "We will explain the following concepts as they relate to this workflow:\n",
    "- What is an Element versus a Workflow?\n",
    "- Plot the workflow with `dj.Diagram`\n",
    "- Insert data into tables\n",
    "- Query table contents\n",
    "- Fetch table contents\n",
    "- Run the workflow for your experiments\n",
    "\n",
    "For detailed documentation and tutorials on general DwataJoint principles that support collaboration, automation, reproducibility, and visualizations:\n",
    "\n",
    "- [DataJoint Interactive Tutorials](https://github.com/datajoint/datajoint-tutorials) - Fundamentals including table tiers, query operations, fetch operations, automated computations with the `make` function, etc.\n",
    "\n",
    "- [DataJoint Core - Documentation](https://datajoint.com/docs/core/) - Relational data model principles\n",
    "\n",
    "- [DataJoint API for Python - Documentation](https://datajoint.com/docs/core/datajoint-python/)\n",
    "\n",
    "- [DataJoint Element for Volumetric Calcium Imaging - Documentation](https://datajoint.com/docs/elements/element-zstack/)\n",
    "\n",
    "Let's start by importing the packages necessary to run this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DataJoint Workflow for volumetric Calcium Imaging is assembled from 5 DataJoint Elements\n",
    "\n",
    "| Element | Source Code | Documentation | Description |\n",
    "| -- | -- | -- | -- |\n",
    "| Element Lab | [Link](https://github.com/datajoint/element-lab) | [Link](https://datajoint.com/docs/elements/element-lab) | Lab management related information, such as Lab, User, Project, Protocol, Source. |\n",
    "| Element Animal | [Link](https://github.com/datajoint/element-animal) | [Link](https://datajoint.com/docs/elements/element-animal) | General animal metadata and surgery information. |\n",
    "| Element Session | [Link](https://github.com/datajoint/element-session) | [Link](https://datajoint.com/docs/elements/element-session) | General information of experimental sessions. |\n",
    "| Element Calcium Imaging | [Link](https://github.com/datajoint/element-calcium-imaging) | [Link](https://datajoint.com/docs/elements/element-calcium-imaging) |  General information about the calcium imaging scan. |\n",
    "| Element zstack | [Link](https://github.com/datajoint/element-zstack) | [Link](https://datajoint.com/docs/elements/element-zstack) |  Volumetric data segmentation and export. |\n",
    "\n",
    "Each workflow is composed of multiple Elements. Each Element contains 1 or more modules, and each module declares its own schema in the database.\n",
    "\n",
    "The Elements are imported within the `workflow_zstack.pipeline` script.\n",
    "\n",
    "By importing the modules for the first time, the schemas and tables will be created in the database.  Once created, importing modules will not create schemas and tables again, but the existing schemas/tables can be accessed.\n",
    "\n",
    "The schema diagram (shown below) is a good reference for understanding the order of the tables within the workflow.\n",
    "\n",
    "Let's activate the Elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow_zstack.pipeline import (\n",
    "    lab,\n",
    "    subject,\n",
    "    session,\n",
    "    scan,\n",
    "    volume,\n",
    "    volume_matching,\n",
    "    bossdb,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagram\n",
    "\n",
    "We can plot the diagram of tables within multiple schemas and their dependencies using `dj.Diagram()`.  For details, see the [documentation](https://datajoint.com/docs/core/concepts/getting-started/diagrams/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(scan.Scan)\n",
    "    + dj.Diagram(volume)\n",
    "    + dj.Diagram(volume_matching)\n",
    "    + dj.Diagram(bossdb)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the diagram above seems complex at first, it becomes more clear when it's approached as a hierarchy of tables that define the order in which the workflow expects to receive data in each of its tables.\n",
    "\n",
    "The tables higher up in the diagram such as `subject.Subject()` should be the first to receive data.\n",
    "\n",
    "Data is manually entered into the green, rectangular tables with the `insert1()` method.\n",
    "\n",
    "Tables connected by a solid line depend on entries from the table above it.\n",
    "\n",
    "There are 5 table tiers in DataJoint. Some of these tables appear in the diagram above.\n",
    "\n",
    "| Table tier | Color and shape | Description |\n",
    "| -- | -- | -- |\n",
    "| Manual table | Green box | Data entered from outside the pipeline, either by hand or with external helper scripts. |\n",
    "| Lookup table | Gray box | Small tables containing general facts and settings of the data pipeline; not specific to any experiment or dataset. |  \n",
    "| Imported table | Blue oval | Data ingested automatically inside the pipeline but requiring access to data outside the pipeline. |\n",
    "| Computed table | Red circle | Data computed automatically entirely inside the pipeline. |\n",
    "| Part table | Plain text | Part tables share the same tier as their master table. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert entries into manual tables\n",
    "\n",
    "In this section, we will insert metadata about an animal subject, experiment session, and optogenetic stimulation parameters.\n",
    "\n",
    "Let's start with the first schema and table in the schema diagram (i.e. `subject.Subject` table).\n",
    "\n",
    "Each module (e.g. `subject`) contains a schema object that enables interaction with the schema in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table classes in the module correspond to a table in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the table dependencies and the attributes we need to insert by using the functions `.describe()` and `.heading`.  The `describe()` function displays the table definition with foreign key references and the `heading` function displays the attributes of the table definition.  These are particularly useful functions if you are new to DataJoint Elements and are unsure of the attributes required for each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject.Subject.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will insert data into the `subject.Subject` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(\n",
    "        subject=\"subject1\",\n",
    "        sex=\"M\",\n",
    "        subject_birth_date=\"2023-01-01\",\n",
    "        subject_description=\"Cellpose segmentation of volumetric data.\",\n",
    "    )\n",
    ")\n",
    "subject.Subject()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue inserting in the other manual tables. The `Session` table is next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.Session.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show the dependencies and attributes for the `session.Session` table.\n",
    "\n",
    "Notice that `describe` shows the dependencies of the table on upstream tables (i.e. foreign key references). The `Session` table depends on the upstream `Subject` table. \n",
    "\n",
    "Whereas `heading` lists all the attributes of the `Session` table, regardless of\n",
    "whether they are declared in an upstream table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(\n",
    "    subject=\"subject1\",\n",
    "    session_id=0,\n",
    ")\n",
    "session.Session.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        session_datetime=datetime.datetime.now(),\n",
    "    ),\n",
    ")\n",
    "session.Session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SessionDirectory` table locates the relevant data files in a directory path\n",
    "relative to the root directory defined in your `dj.config[\"custom\"]`. More\n",
    "information about `dj.config` is provided at the end of this tutorial and is\n",
    "particularly useful for local deployments of this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.SessionDirectory.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(\n",
    "    dict(session_key, session_dir=\"subject1/session1\"),\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "session.SessionDirectory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each volume requires an entry in the `Scan` table from\n",
    "`element-calcium-imaging`. Here, we'll use `describe` and `heading` for the Scan\n",
    "table and insert an entry for the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scan.Scan.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.Scan.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.Scan.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        scan_id=0,\n",
    "        acq_software=\"ScanImage\",\n",
    "    ),\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "scan_key = (scan.Scan & \"subject = 'subject1'\").fetch1(\"KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate\n",
    "\n",
    "### Automatically populate tables\n",
    "\n",
    "`volume.Volume` is the first table in the pipeline that can be populated automatically.\n",
    "If a table contains a part table, this part table is also populated during the\n",
    "`populate()` call. `populate()` takes several arguments including a session\n",
    "key. This key restricts `populate()` to performing the operation on the session\n",
    "of interest rather than all possible sessions which could be a time-intensive\n",
    "process for databases with lots of entries.\n",
    "\n",
    "Let's view the `volume.Volume` and populate it using the `populate()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Volume.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Volume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Volume.populate(scan_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information was entered into this table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Volume()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to perform volume segmentation with `cellpose`. An important step before\n",
    "processing is managing the parameters which will be used in that step. To do so, we will\n",
    "insert parameters required by cellpose into a DataJoint table\n",
    "`SegmentationParamSet`. This table keeps track of all combinations of your image\n",
    "processing parameters. You can choose which parameters are used during\n",
    "processing in a later step.\n",
    "\n",
    "Let's view the attributes and insert data into `volume.SegmentationParamSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.SegmentationParamSet.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.SegmentationParamSet.insert_new_params(\n",
    "    segmentation_method=\"cellpose\",\n",
    "    paramset_idx=1,\n",
    "    params=dict(\n",
    "        diameter=8,\n",
    "        min_size=2,\n",
    "        do_3d=False,\n",
    "        anisotropy=0.5,\n",
    "        model_type=\"nuclei\",\n",
    "        channels=[[0, 0]],\n",
    "        z_axis=0,\n",
    "        skip_duplicates=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataJoint uses a `SegmentationTask` table to\n",
    "manage which `Volume` and `SegmentationParamSet` should be used during processing. \n",
    "\n",
    "This table is important for defining several important aspects of\n",
    "downstream processing. Let's view the attributes to get a better understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(volume.SegmentationTask.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.SegmentationTask.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SegmentationTask` table contains two important attributes: \n",
    "+ `paramset_idx`\n",
    "+ `task_mode`\n",
    "\n",
    "The `paramset_idx` attribute is tracks\n",
    "your segmentation parameter sets. You can choose the parameter set on which\n",
    "you want to run segmentation analysis based on this attribute. This\n",
    "attribute tells the `Segmentation` table which set of parameters you are\n",
    "processing in a given `populate()`.\n",
    "\n",
    "The `task_mode` attribute can be set to either `load` or `trigger`. When set to `trigger`, the\n",
    "segmentation step will run cellpose on the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.SegmentationTask.insert1(\n",
    "    dict(\n",
    "        scan_key,\n",
    "        paramset_idx=1,\n",
    "        task_mode=\"trigger\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, Element ZStack only supports triggering cellpose. Now, we can popluate\n",
    "the `Segmentation` table. This step may take several hours, depending on your\n",
    "computer's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Segmentation.populate(scan_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volumetric data uploaded to BossDB requires information about voxel size. The\n",
    "DataJoint table `volume.VoxelSize` can be used to insert this information for a\n",
    "given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.VoxelSize.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.VoxelSize.insert1(dict(scan_key, width=0.001, height=0.001, depth=0.001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an `upload_key` to easily upload the volume to BossDB via this\n",
    "workflow. The `upload_key` combines information about the current scan from\n",
    "`scan.Scan` and the `paramset_idx` from `SegmentationParamSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_key = dict(scan_key, paramset_idx=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can upload the volume and its corresponding segmentation data to\n",
    "BossDB and generate a neuroglancer link to visualize the data ..\n",
    "\n",
    "The first table is `VolumeUploadTask`. Let's define the upload task by naming the collection, experiment,\n",
    "and channel where the data should be uploaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bossdb.VolumeUploadTask.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bossdb.VolumeUploadTask.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = \"dataJointTestUpload\"\n",
    "exp_name = \"CaImagingFinal\"\n",
    "chn_name = \"test1-seg\"\n",
    "bossdb.VolumeUploadTask.insert1(\n",
    "    dict(\n",
    "        upload_key,\n",
    "        collection_name=col_name,\n",
    "        experiment_name=exp_name,\n",
    "        channel_name=chn_name,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can upload data. \n",
    "\n",
    "As a reminder, uploading data to BossDB via this pipeline requires an API\n",
    "token which can be obtained by creating an account at\n",
    "[api.bossdb.io](https://api.bossdb.io). You will also need resource manager\n",
    "permissions from the team at [BossDB](https://bossdb.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bossdb.VolumeUpload.populate(upload_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the volumetric data, import the neuroglancer URL and paste it into\n",
    "your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bossdb.VolumeUpload.WebAddress & upload_key & \"upload_type='image+annotation'\").fetch1(\n",
    "    \"web_address\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ele')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d00c4ad21a7027bf1726d6ae3a9a6ef39c8838928eca5a3d5f51f3eb68720410"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
