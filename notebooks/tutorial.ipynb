{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Process volumetric fluorescent microscopy data with DataJoint Elements\n",
    "\n",
    "This notebook will walk through processing volumetric two-photon calcium imaging data collected\n",
    "from ScanImage and segmented with cellpose. While anyone can work through this\n",
    "notebook to process volumetric fluorescent microscopy data through DataJoint's\n",
    "`element-zstack` pipeline, for a detailed tutorial about the fundamentals of\n",
    "DataJoint including table types, make functions, and querying, please see the\n",
    "[DataJoint Tutorial](https://github.com/datajoint/datajoint-tutorials).\n",
    "\n",
    "**Please note that uploading data to BossDB via this pipeline requires an API\n",
    "token which can be obtained by creating an account at\n",
    "[api.bossdb.io](https://api.bossdb.io). You will also need resource manager\n",
    "permissions from the team at [BossDB](https://bossdb.org).**\n",
    "\n",
    "The DataJoint Python API and Element ZStack offer a lot of features to\n",
    "support collaboration, automation, reproducibility, and visualizations.\n",
    "For more information on these topics, please visit our documentation: \n",
    " \n",
    "- [DataJoint Core](https://datajoint.com/docs/core/): General principles\n",
    "\n",
    "- DataJoint [Python](https://datajoint.com/docs/core/datajoint-python/) and\n",
    "  [MATLAB](https://datajoint.com/docs/core/datajoint-matlab/) APIs: in-depth reviews of\n",
    "  specifics\n",
    "\n",
    "- [DataJoint Element ZStack](https://datajoint.com/docs/elements/element-zstack/):\n",
    "  A modular pipeline for volumetric calcium imaging data analysis\n",
    "\n",
    "\n",
    "Let's start by importing the packages necessary to run this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basics:\n",
    "\n",
    "Any DataJoint workflow can be broken down into basic 3 parts:\n",
    "\n",
    "- `Insert`\n",
    "- `Populate` (or process)\n",
    "- `Query`\n",
    "\n",
    "In this demo we will:\n",
    "- `Insert` metadata about an animal subject, recording session, and \n",
    "  parameters related to processing calcium imaging data through Suite2p.\n",
    "- `Populate` tables with outputs of image processing including motion correction,\n",
    "  segmentation, mask classification, fluorescence traces and deconvolved activity traces.\n",
    "- `Query` the processed data from the database and plot calcium activity traces.\n",
    "\n",
    "Each of these topics will be explained thoroughly in this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow diagram\n",
    "\n",
    "This workflow is assembled from 5 DataJoint elements:\n",
    "+ [element-lab](https://github.com/datajoint/element-lab)\n",
    "+ [element-animal](https://github.com/datajoint/element-animal)\n",
    "+ [element-session](https://github.com/datajoint/element-session)\n",
    "+ [element-calcium-imaging](https://github.com/datajoint/element-calcium-imaging)\n",
    "+ [element-zstack](https://github.com/datajoint/element-zstack)\n",
    "\n",
    "Each element declares its own schema in the database. These schemas can be imported like\n",
    "any other Python package. This workflow is composed of schemas from each of the Elements\n",
    "above and correspond to a module within `workflow_zstack.pipeline`.\n",
    "\n",
    "The schema diagram is a good reference for understanding the order of the tables\n",
    "within the workflow, as well as the corresponding table type.\n",
    "Let's activate the elements and view the schema diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow_zstack.pipeline import (\n",
    "    lab,\n",
    "    subject,\n",
    "    session,\n",
    "    scan,\n",
    "    volume,\n",
    "    volume_matching,\n",
    "    bossdb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    dj.Diagram(subject.Subject)\n",
    "    + dj.Diagram(session.Session)\n",
    "    + dj.Diagram(scan.Scan)\n",
    "    + dj.Diagram(volume)\n",
    "    + dj.Diagram(volume_matching)\n",
    "    + dj.Diagram(bossdb)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagram Breakdown\n",
    "\n",
    "While the diagram above seems complex at first, it becomes more clear when it's\n",
    "approached as a hierarchy of tables that **define the order** in which the\n",
    "workflow **expects to receive data** in each of its tables. \n",
    "\n",
    "- Tables with a green, or rectangular shape expect to receive data manually using the\n",
    "`insert()` function. \n",
    "- The tables higher up in the diagram such as `subject.Subject()`\n",
    "should be the first to receive data. This ensures data integrity by preventing orphaned\n",
    "data within DataJoint schemas. \n",
    "- Tables with a purple oval or red circle can be automatically filled with relevant data\n",
    "  by calling `populate()`. For example `volume.Segmentation` and its part-table\n",
    "  `volume.Segmentation.Mask` are both populated with `volume.Segmentation.populate()`.\n",
    "- Tables connected by a solid line depend on attributes (entries) in the table\n",
    "  above it.\n",
    "\n",
    "#### Table Types\n",
    "\n",
    "There are 5 table types in DataJoint. Each of these appear in the diagram above.\n",
    "\n",
    "- **Manual table**: green box, manually inserted table, expect new entries daily, e.g. `Subject`, `Scan`.  \n",
    "- **Lookup table**: gray box, pre inserted table, commonly used for general facts or parameters. e.g. `bossdb.UploadParamset`, `volume.SegmentationParamset`.  \n",
    "- **Imported table**: blue oval, auto-processing table, the processing depends\n",
    "  on the importing of external files. e.g. process of obtaining the `Volume` data requires\n",
    "  raw data stored outside the database.  \n",
    "- **Computed table**: red circle, auto-processing table, the processing does not\n",
    "  depend on files external to the database, commonly used for computations such\n",
    "  as `volume.Segmentation`, `volume_match.VolumeMatch`.   \n",
    "- **Part table**: plain text, as an appendix to the master table, all the part\n",
    "  entries of a given master entry represent a intact set of the master entry.\n",
    "  e.g. Masks of `Segmentation`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the workflow: Insert\n",
    "\n",
    "### Insert entries into manual tables\n",
    "\n",
    "To view details about a table's dependencies and attributes, use functions `.describe()`\n",
    "and `.heading`, respectively.\n",
    "\n",
    "Let's start with the first table in the schema diagram (the `subject` table) and view\n",
    "the table attributes we need to insert. There are two ways you can do this: *run each\n",
    "of the two cells below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject.Subject.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(\n",
    "        subject=\"subject1\",\n",
    "        sex=\"M\",\n",
    "        subject_birth_date=\"2023-01-01\",\n",
    "        subject_description=\"Cellpose segmentation of volumetric data.\",\n",
    "    )\n",
    ")\n",
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table and see how the output varies between `.describe` and `.heading`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.Session.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show the dependencies and attributes for the `session.Session` table.\n",
    "Notice that `describe` shows the dependencies of the table on upstream tables. The\n",
    "`Session` table depends on the upstream `Subject` table. \n",
    "\n",
    "Whereas `heading` lists all the attributes of the `Session` table, regardless of\n",
    "whether they are declared in an upstream table. \n",
    "\n",
    "Here we will demonstrate a very useful way of inserting data by assigning the dictionary\n",
    "to a variable `session_key`. This variable can be used to insert entries into tables that\n",
    "contain the `Session` table as one of its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(\n",
    "    subject=\"subject1\",\n",
    "    session_id=0,\n",
    ")\n",
    "session.Session.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        session_datetime=datetime.datetime.now(),\n",
    "    ),\n",
    ")\n",
    "session.Session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SessionDirectory` table locates the relevant data files in a directory path\n",
    "relative to the root directory defined in your `dj.config[\"custom\"]`. More\n",
    "information about `dj.config` is provided at the end of this tutorial and is\n",
    "particularly useful for local deployments of this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.SessionDirectory.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(\n",
    "    dict(session_key, session_dir=\"sub1\"),\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "session.SessionDirectory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each volume requires an entry in the `Scan` table from\n",
    "`element-calcium-imaging`. Here, we'll use `describe` and `heading` for the Scan\n",
    "table and insert an entry for the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scan.Scan.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.Scan.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.Scan.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        scan_id=0,\n",
    "        acq_software=\"ScanImage\",\n",
    "    ),\n",
    "    skip_duplicates=True,\n",
    ")\n",
    "scan_key = (scan.Scan & \"subject = 'subject1'\").fetch1(\"KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate\n",
    "\n",
    "### Automatically populate tables\n",
    "\n",
    "`volume.Volume` is the first table in the pipeline that can be populated automatically.\n",
    "If a table contains a part table, this part table is also populated during the\n",
    "`populate()` call. `populate()` takes several arguments including the a session\n",
    "key. This key restricts `populate()` to performing the operation on the session\n",
    "of interest rather than all possible sessions which could be a time-intensive\n",
    "process for databases with lots of entries.\n",
    "\n",
    "Let's view the `volume.Volume` and populate it using the `populate()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Volume.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Volume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Volume.populate(scan_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information was entered into this table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Volume()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to perform volume segmentation with `cellpose`. An important step before\n",
    "processing is managing the parameters which will be used in that step. To do so, we will\n",
    "insert parameters required by cellpose into a DataJoint table\n",
    "`SegmentationParamSet`. This table keeps track of all combinations of your image\n",
    "processing parameters. You can choose which parameters are used during\n",
    "processing in a later step.\n",
    "\n",
    "Let's view the attributes and insert data into `volume.SegmentationParamSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.SegmentationParamSet.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.SegmentationParamSet.insert_new_params(\n",
    "    segmentation_method=\"cellpose\",\n",
    "    paramset_idx=1,\n",
    "    params=dict(\n",
    "        diameter=8,\n",
    "        min_size=2,\n",
    "        do_3d=False,\n",
    "        anisotropy=0.5,\n",
    "        model_type=\"nuclei\",\n",
    "        channels=[[0, 0]],\n",
    "        z_axis=0,\n",
    "        skip_duplicates=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've inserted cellpose parameters into the `SegmentationParamSet` table,\n",
    "we're almost ready to run image processing. DataJoint uses a `SegmentationTask` table to\n",
    "manage which `Volume` and `SegmentationParamSet` should be used during processing. \n",
    "\n",
    "This table is important for defining several important aspects of\n",
    "downstream processing. Let's view the attributes to get a better understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(volume.SegmentationTask.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.SegmentationTask.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SegmentationTask` table contains two important attributes: \n",
    "+ `paramset_idx`\n",
    "+ `task_mode`\n",
    "\n",
    "The `paramset_idx` attribute is tracks\n",
    "your segmentation parameter sets. You can choose the parameter set on which\n",
    "you want to run segmentation analysis based on this attribute. This\n",
    "attribute tells the `Segmentation` table which set of parameters you are\n",
    "processing in a given `populate()`.\n",
    "\n",
    "The `task_mode` attribute can be set to either `load` or `trigger`. When set to `trigger`, the\n",
    "segmentation step will run cellpose on the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.SegmentationTask.insert1(\n",
    "    dict(\n",
    "        scan_key,\n",
    "        paramset_idx=1,\n",
    "        task_mode=\"trigger\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, Element ZStack only supports triggering cellpose. Now, we can popluate\n",
    "the `Segmentation` table. This step may take several hours, depending on your\n",
    "computer's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.Segmentation.populate(scan_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can upload our data to BossDB. The `bossdb` schema contains two\n",
    "tables to the upload tasks, and execute the upload. The structure of these\n",
    "tables mirrors the `volume` schema.\n",
    "\n",
    "Volumetric data uploaded to BossDB requires information about voxel size. The\n",
    "DataJoint table `volume.VoxelSize` can be used to insert this information for a\n",
    "given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.VoxelSize.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.VoxelSize.insert1(dict(scan_key, width=0.001, height=0.001, depth=0.001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the upload task by naming the collection, experiment,\n",
    "and channel where the data should be uploaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bossdb.VolumeUploadTask.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bossdb.VolumeUploadTask.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = \"dataJointTestUpload\"\n",
    "exp_name = \"CaImagingFinal\"\n",
    "chn_name = \"test1-seg\"\n",
    "bossdb.VolumeUploadTask.insert1(\n",
    "    dict(\n",
    "        scan_key,\n",
    "        collection_name=col_name,\n",
    "        experiment_name=exp_name,\n",
    "        channel_name=chn_name,\n",
    "        upload_type=\"annotation\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bossdb.VolumeUploadTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_key = (bossdb.VolumeUploadTask & scan_key & \"upload_type = 'image'\").fetch(\"KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bossdb.BossDBURLs.populate(upload_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bossdb.BossDBURLs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the volumetric data, import the neuroglancer URL and paste it into\n",
    "your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bossdb.BossDBURLs & scan_key).fetch1(\"neuroglancer_url\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how to get permission from APL to upload data. \n",
    "\n",
    "Create a schema to automatically generate neuroglancer link and insert into DJ\n",
    "table. \n",
    "\n",
    "Include BossDBUpload in BossDBURLs as a computed/imported table. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ele')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d00c4ad21a7027bf1726d6ae3a9a6ef39c8838928eca5a3d5f51f3eb68720410"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
